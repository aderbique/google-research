{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yAq6aHVh5oCH"
   },
   "source": [
    "**Copyright 2020 Google LLC.**\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpEpTFph2ysp"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os,sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-u1ecNmMiX3"
   },
   "source": [
    "## Overview\n",
    "\n",
    "### Pre-processes UCI Adult (Census Income) dataset:\n",
    "\n",
    "Download the Adult train and test data files can be downloaded from:\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
    "and save them in the `./group_agnostic_fairness/data/uci_adult` folder.\n",
    "\n",
    "Input: \n",
    "\n",
    "*   ./group_agnostic_fairness/data/uci_adult/adult.data \n",
    "*   ./group_agnostic_fairness/data/uci_adult/adult.test\n",
    "\n",
    "\n",
    "\n",
    "Outputs: train.csv, test.csv, mean_std.json, vocabulary.json, IPS_exampleweights_with_label.json, IPS_exampleweights_without_label.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oyFyRbFk7zox"
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "dataset_base_dir = '../../group_agnostic_fairness/data/uci_adult/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgWxzZeyKog3"
   },
   "source": [
    "### Load original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hB-PxNRyCRZm"
   },
   "outputs": [],
   "source": [
    "def convert_object_type_to_category(df):\n",
    "  \"\"\"Converts columns of type object to category.\"\"\"\n",
    "  df = pd.concat([df.select_dtypes(include=[], exclude=['object']),\n",
    "                  df.select_dtypes(['object']).apply(pd.Series.astype, dtype='category')\n",
    "                  ], axis=1).reindex(df.columns, axis=1)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chhYs8357xyU"
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = os.path.join(dataset_base_dir,'adult.data')\n",
    "TEST_FILE = os.path.join(dataset_base_dir,'adult.test')\n",
    "\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "target_variable = \"income\"\n",
    "target_value = \">50K\"\n",
    "\n",
    "with open(TRAIN_FILE, \"r\") as TRAIN_FILE:\n",
    "  train_df = pd.read_csv(TRAIN_FILE,sep=',',names=columns)\n",
    "\n",
    "with open(TEST_FILE, \"r\") as TEST_FILE:\n",
    "  test_df = pd.read_csv(TEST_FILE,sep=',',names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wy_GkVwHUsKQ"
   },
   "outputs": [],
   "source": [
    "# Convert columns of type ``object`` to ``category`` \n",
    "train_df = convert_object_type_to_category(train_df)\n",
    "test_df = convert_object_type_to_category(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VQE85STLL46"
   },
   "source": [
    "### Computing Invese propensity weights for each subgroup, and writes to directory.\n",
    "\n",
    "IPS_example_weights_with_label.json: json dictionary of the format\n",
    "        {subgroup_id : inverse_propensity_score,...}. Used by IPS_reweighting_model approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "id": "2fkieHul02TL",
    "outputId": "cb4c15dc-1979-46ee-c4c1-7ff546d30907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 0, 2: 0, 3: 0}\n"
     ]
    }
   ],
   "source": [
    "IPS_example_weights_without_label = {\n",
    "  0: (len(train_df))/(len(train_df[(train_df.race != 'Black') & (train_df.sex != 'Female')])), # 00: White Male\n",
    "}\n",
    "try:\n",
    "  z = (len(train_df))/(len(train_df[(train_df.race != 'Black') & (train_df.sex == 'Female')])) # 01: White Female\n",
    "except ZeroDivisionError:\n",
    "  z = 0\n",
    "IPS_example_weights_without_label[1] = z\n",
    "\n",
    "try:\n",
    "  z = (len(train_df))/(len(train_df[(train_df.race == 'Black') & (train_df.sex != 'Female')])) # 10: Black Male\n",
    "except ZeroDivisionError:\n",
    "  z = 0\n",
    "IPS_example_weights_without_label[2] = z\n",
    "\n",
    "try:\n",
    "  z = (len(train_df))/(len(train_df[(train_df.race == 'Black') & (train_df.sex == 'Female')]))  # 11: Black Female\n",
    "except ZeroDivisionError:\n",
    "  z = 0\n",
    "IPS_example_weights_without_label[3] = z\n",
    "\n",
    "output_file_path = os.path.join(dataset_base_dir,'IPS_example_weights_without_label.json')\n",
    "with open(output_file_path, mode=\"w\") as output_file:\n",
    "    output_file.write(json.dumps(IPS_example_weights_without_label))\n",
    "    output_file.close()\n",
    "\n",
    "print(IPS_example_weights_without_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "id": "Dm15uo-R0-LB",
    "outputId": "15fd3cf8-3feb-48f7-dcbf-228fe02f7dbc"
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m IPS_example_weights_with_label \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m \u001b[38;5;241m0\u001b[39m: (\u001b[38;5;28mlen\u001b[39m(train_df))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_df[(train_df[target_variable] \u001b[38;5;241m!=\u001b[39m target_value) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39mrace \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlack\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39msex \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m)])), \u001b[38;5;66;03m# 000: Negative White Male\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;241m1\u001b[39m: \u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_variable\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFemale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;66;03m# 001: Negative White Female\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;241m2\u001b[39m: (\u001b[38;5;28mlen\u001b[39m(train_df))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_df[(train_df[target_variable] \u001b[38;5;241m!=\u001b[39m target_value) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39mrace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlack\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39msex \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m)])), \u001b[38;5;66;03m# 010: Negative Black Male\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;241m3\u001b[39m: (\u001b[38;5;28mlen\u001b[39m(train_df))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_df[(train_df[target_variable] \u001b[38;5;241m!=\u001b[39m target_value) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39mrace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlack\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39msex \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m)])), \u001b[38;5;66;03m# 011: Negative Black Female\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;241m4\u001b[39m: (\u001b[38;5;28mlen\u001b[39m(train_df))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_df[(train_df[target_variable] \u001b[38;5;241m==\u001b[39m target_value) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39mrace \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlack\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39msex \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m)])), \u001b[38;5;66;03m# 100: Positive White Male\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;241m5\u001b[39m: (\u001b[38;5;28mlen\u001b[39m(train_df))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_df[(train_df[target_variable] \u001b[38;5;241m==\u001b[39m target_value) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39mrace \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlack\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39msex \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m)])), \u001b[38;5;66;03m# 101: Positive White Female\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;241m6\u001b[39m: (\u001b[38;5;28mlen\u001b[39m(train_df))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_df[(train_df[target_variable] \u001b[38;5;241m==\u001b[39m target_value) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39mrace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlack\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39msex \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m)])), \u001b[38;5;66;03m# 110: Positive Black Male\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;241m7\u001b[39m: (\u001b[38;5;28mlen\u001b[39m(train_df))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_df[(train_df[target_variable] \u001b[38;5;241m==\u001b[39m target_value) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39mrace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlack\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (train_df\u001b[38;5;241m.\u001b[39msex \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m)])), \u001b[38;5;66;03m# 111: Positive Black Female\u001b[39;00m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     12\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_base_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIPS_example_weights_with_label.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "IPS_example_weights_with_label = {}\n",
    "try:\n",
    "    z = (len(train_df))/(len(train_df[(train_df[target_variable] != target_value) & (train_df.race != 'Black') & (train_df.sex != 'Female')])), # 000: Negative White Male\n",
    "except ZeroDivisionError:\n",
    "    z = 0\n",
    "IPS_example_weights_with_label[0] = z\n",
    "\n",
    "try:\n",
    "    z = (len(train_df))/(len(train_df[(train_df[target_variable] != target_value) & (train_df.race == 'Black') & (train_df.sex != 'Female')])) # 010: Negative Black Male\n",
    "except ZeroDivisionError:\n",
    "    z = 0\n",
    "IPS_example_weights_with_label[1] = z\n",
    "\n",
    "try:\n",
    "    z = (len(train_df))/(len(train_df[(train_df[target_variable] != target_value) & (train_df.race == 'Black') & (train_df.sex != 'Female')])) # 010: Negative Black Male\n",
    "except ZeroDivisionError:\n",
    "    z = 0\n",
    "IPS_example_weights_with_label[2] = z\n",
    "\n",
    "try:\n",
    "    z = (len(train_df))/(len(train_df[(train_df[target_variable] != target_value) & (train_df.race == 'Black') & (train_df.sex == 'Female')])) # 011: Negative Black Female\n",
    "except ZeroDivisionError:\n",
    "    z = 0\n",
    "IPS_example_weights_with_label[3] = z\n",
    "\n",
    "try:\n",
    "    z = (len(train_df))/(len(train_df[(train_df[target_variable] == target_value) & (train_df.race != 'Black') & (train_df.sex != 'Female')])) # 100: Positive White Male\n",
    "except ZeroDivisionError:\n",
    "    z = 0\n",
    "IPS_example_weights_with_label[4] = z\n",
    "\n",
    "try:\n",
    "    z = (len(train_df))/(len(train_df[(train_df[target_variable] == target_value) & (train_df.race != 'Black') & (train_df.sex == 'Female')])) # 101: Positive White Female\n",
    "except ZeroDivisionError:\n",
    "    z = 0\n",
    "IPS_example_weights_with_label[5] = z\n",
    "\n",
    "try:\n",
    "    z = (len(train_df))/(len(train_df[(train_df[target_variable] == target_value) & (train_df.race == 'Black') & (train_df.sex != 'Female')])) # 110: Positive Black Male\n",
    "except ZeroDivisionError:\n",
    "    z = 0\n",
    "IPS_example_weights_with_label[6] = z\n",
    "\n",
    "try:\n",
    "    z = AAA\n",
    "except ZeroDivisionError:\n",
    "    z = 0\n",
    "IPS_example_weights_with_label[7] = z\n",
    "\n",
    "2: \n",
    "3: \n",
    "4: \n",
    "5: \n",
    "6: \n",
    "7: (len(train_df))/(len(train_df[(train_df[target_variable] == target_value) & (train_df.race == 'Black') & (train_df.sex == 'Female')])), # 111: Positive Black Female\n",
    "}\n",
    "  \n",
    "output_file_path = os.path.join(dataset_base_dir,'IPS_example_weights_with_label.json')\n",
    "with open(output_file_path, mode=\"w\") as output_file:\n",
    "    output_file.write(json.dumps(IPS_example_weights_with_label))\n",
    "    output_file.close()\n",
    "\n",
    "print(IPS_example_weights_with_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SQc7h9HLcSc"
   },
   "source": [
    "### Construct vocabulary.json, and write to directory.\n",
    "\n",
    "vocabulary.json: json dictionary of the format {feature_name:      [feature_vocabulary]}, containing vocabulary for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 54
    },
    "colab_type": "code",
    "id": "YIebJG2YfMpv",
    "outputId": "3c38fa2e-da0b-4958-915f-d3990556c138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'workclass': ['Self-emp-inc', 'Self-emp-not-inc', 'Federal-gov', 'Never-worked', 'State-gov', 'Without-pay', 'Private', 'Local-gov'], 'education': ['HS-grad', 'Doctorate', 'Masters', 'Assoc-voc', '1st-4th', '11th', '5th-6th', 'Assoc-acdm', 'Some-college', '10th', '7th-8th', '9th', 'Preschool', '12th', 'Bachelors', 'Prof-school'], 'marital-status': ['Divorced', 'Married-AF-spouse', 'Married-civ-spouse', 'Never-married', 'Married-spouse-absent', 'Widowed', 'Separated'], 'occupation': ['Tech-support', 'Farming-fishing', 'Craft-repair', 'Other-service', 'Exec-managerial', 'Sales', 'Handlers-cleaners', 'Priv-house-serv', 'Prof-specialty', 'Adm-clerical', 'Armed-Forces', 'Protective-serv', 'Machine-op-inspct', 'Transport-moving'], 'relationship': ['Other-relative', 'Wife', 'Husband', 'Own-child', 'Not-in-family', 'Unmarried'], 'race': ['Amer-Indian-Eskimo', 'Other', 'White', 'Asian-Pac-Islander', 'Black'], 'sex': ['Female', 'Male'], 'native-country': ['Iran', 'Ireland', 'Japan', 'Germany', 'Portugal', 'Greece', 'Mexico', 'Thailand', 'Outlying-US(Guam-USVI-etc)', 'Columbia', 'Philippines', 'France', 'Scotland', 'Hungary', 'Vietnam', 'India', 'Puerto-Rico', 'Hong', 'Poland', 'Nicaragua', 'Canada', 'China', 'Jamaica', 'Italy', 'Haiti', 'Ecuador', 'United-States', 'Holand-Netherlands', 'Cambodia', 'Peru', 'Honduras', 'Dominican-Republic', 'Trinadad&Tobago', 'Yugoslavia', 'South', 'Taiwan', 'Cuba', 'El-Salvador', 'England', 'Laos', 'Guatemala'], 'income': ['>50K', '<=50K']}\n"
     ]
    }
   ],
   "source": [
    "cat_cols = train_df.select_dtypes(include='category').columns\n",
    "vocab_dict = {}\n",
    "for col in cat_cols:\n",
    "  vocab_dict[col] = list(set(train_df[col].cat.categories)-{\"?\"})\n",
    "  \n",
    "output_file_path = os.path.join(dataset_base_dir,'vocabulary.json')\n",
    "with open(output_file_path, mode=\"w\") as output_file:\n",
    "    output_file.write(json.dumps(vocab_dict))\n",
    "    output_file.close()\n",
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9cxiG9SLfk6"
   },
   "source": [
    "### Construct mean_std.json, and write to directory\n",
    "\n",
    "mean_std.json: json dictionary of the format feature_name: [mean, std]},\n",
    "containing mean and std for numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 54
    },
    "colab_type": "code",
    "id": "sUWCDXhaQZE_",
    "outputId": "82d48f27-6ced-41ea-ff94-60d41d6561a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': [38.58164675532078, 13.640432553581341], 'fnlwgt': [189778.36651208502, 105549.97769702224], 'education-num': [10.0806793403151, 2.5727203320673877], 'capital-gain': [1077.6488437087312, 7385.292084840338], 'capital-loss': [87.303829734959, 402.9602186489998], 'hours-per-week': [40.437455852092995, 12.347428681731843]}\n"
     ]
    }
   ],
   "source": [
    "temp_dict = train_df.describe().to_dict()\n",
    "mean_std_dict = {}\n",
    "for key, value in temp_dict.items():\n",
    "  mean_std_dict[key] = [value['mean'],value['std']]\n",
    "\n",
    "output_file_path = os.path.join(dataset_base_dir,'mean_std.json')\n",
    "with open(output_file_path, mode=\"w\") as output_file:\n",
    "    output_file.write(json.dumps(mean_std_dict))\n",
    "    output_file.close()\n",
    "print(mean_std_dict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CreateUCIAdultDatasetFiles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
